{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UK House Prices on Map\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "This project's scope is to develop a data lake of house sale transactions in the UK.\n",
    "It is designed with data analysts in mind, who need to visualise temporal changes in house prices in different regions of the UK. Currently, the data warehouse combines two data sources: price paid data transactions received at Land Registry during the last 15 years (25M rows), as well as geographical and administrative metadata about UK post codes (1.7M rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: utm in ./venv/lib/python3.7/site-packages (0.5.0)\n",
      "Requirement already satisfied: pyspark in ./venv/lib/python3.7/site-packages (2.4.6)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.7/site-packages (0.25.3)\n",
      "Requirement already satisfied: psycopg2-binary in ./venv/lib/python3.7/site-packages (2.8.5)\n",
      "Requirement already satisfied: py4j==0.10.7 in ./venv/lib/python3.7/site-packages (from pyspark) (0.10.7)\n",
      "Requirement already satisfied: pytz>=2017.2 in ./venv/lib/python3.7/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in ./venv/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in ./venv/lib/python3.7/site-packages (from pandas) (1.18.5)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from spark_jobs import etl_spark\n",
    "\n",
    "# Make the local module discoverable:\n",
    "sys.path.append(os.path.join(os.getcwd(), 'spark_jobs'))\n",
    "\n",
    "\n",
    "!{sys.executable} -m pip install utm pyspark pandas psycopg2-binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The goal of the project is to provide data analysts (e.g. government statisticians preparing annual reports, business analytics in real estate companies using dashboards) with a data platform that focuses on time series of prices and geographical location.\n",
    "For that, we need a dataset of sale transactions and map locations of UK addresses, which we combine from two different open sources.\n",
    "Since the solution needs to allow end users perform complex queries (e.g. analyses by year and location), and the dataset is being continuously generated with substantial size, AWS Redshift is used as a platform for queries, together with Apache Spark for data processing.\n",
    "\n",
    "An example query: \"Visualize the change in average house prices on a UK map\".\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? \n",
    "\n",
    "Currently, we use two data sources:\n",
    "1. **Price Paid Dataset (PPD)** that includes standard and additional price paid data transactions received at UK Land Registry from 1 January 1995 to 1 May 2020. **25,233,169 rows.** Download link: http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-complete.csv\n",
    "    - *Contains HM Land Registry data © Crown copyright and database right 2020. This data is licensed under the Open Government Licence v3.0.*\n",
    "    - The dataset is being updated once a month (useful for maintenance) and is also available for download as a single file.\n",
    "    \n",
    "Data item | Explanation (where appropriate)\n",
    "----------|--------------------------------\n",
    "Transaction unique identifier | A reference number which is generated automatically recording each published sale. The number is unique and will change each time a sale is recorded.\n",
    "Price | Sale price stated on the transfer deed.\n",
    "Date of Transfer | Date when the sale was completed, as stated on the transfer deed.\n",
    "Postcode | This is the postcode used at the time of the original transaction. Note that postcodes can be reallocated and these changes are not reflected in the Price Paid Dataset.\n",
    "Property Type | D = Detached, S = Semi-Detached, T = Terraced, F = Flats/Maisonettes, O = Other\n",
    "Old/New | Indicates the age of the property and applies to all price paid transactions, residential and non-residential. Y = a newly built property, N = an established residential building\n",
    "Duration | Relates to the tenure: F = Freehold, L= Leasehold etc. Note that HM Land Registry does not record leases of 7 years or less in the Price Paid Dataset.\n",
    "PAON | Primary Addressable Object Name. Typically the house number or name.\n",
    "SAON | Secondary Addressable Object Name. Where a property has been divided into separate units (for example, flats), the PAON (above) will identify the building and a SAON will be specified that identifies the separate unit/flat.\n",
    "Street\t\n",
    "Locality\t\n",
    "Town/City\t\n",
    "District\t\n",
    "County\t\n",
    "PPD Category Type | Indicates the type of Price Paid transaction. A = Standard Price Paid entry, includes single residential property sold for value. B = Additional Price Paid entry including transfers under a power of sale/repossessions, buy-to-lets (where they can be identified by a Mortgage) and transfers to non-private individuals.\n",
    "Record Status\t| Monthly file only. Indicates additions, changes and deletions to the records. A = Addition. C = Change. D = Delete.\n",
    "\n",
    "2. **Code-Point Open dataset** that has administrative and geographical metadata about all UK postal codes. **1,706,904 rows.** Download link: https://www.ordnancesurvey.co.uk/business-government/products/code-point-open (needs email address).\n",
    "    - *Contains OS data © Crown copyright and database right 2018*\n",
    "    - *Contains Royal Mail data © Royal Mail copyright and database right 2018*\n",
    "    - *Contains National Statistics data © Crown copyright and database right 2018*\n",
    "\n",
    "Data item | Explanation (where appropriate)\n",
    "----------|--------------------------------\n",
    "Postcode\n",
    "Positional quality indicator | Indicates the quality of the data underlying the Code-Point Open location coordinate (CPLC).\n",
    "Eastings | Eastward-measured British National Grid (BNG) geographic Cartesian coordinate for a point. \n",
    "Northings | Northward-measured British National Grid (BNG) geographic Cartesian coordinate for a point. \n",
    "Country code | Either England, Scotland or Wales.\n",
    "NHS regional HA code | The National Health Service (NHS) region.\n",
    "NHS HA code\n",
    "Admin county code | The local government county code.\n",
    "Admin district code | The local government district code.\n",
    "Admin ward code | The local government ward code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Since both of the data sources contain government data, there are almost no issues with data quality - moreover, the UK Land Registry publishes error corrections in a timely manner. The same applies to the postcodes dataset, which is being updated in case of errors by its author company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"credentials.cfg\")\n",
    "\n",
    "PPD_PATH = config.get(\"S3\", \"SPARK_INPUT_PPD\")\n",
    "PPD_HEADERS = config.get(\"S3\", \"SPARK_INPUT_PPD_HEADERS\")\n",
    "POSTCODE_PATH = config.get(\"S3\", \"SPARK_INPUT_POSTCODES\")\n",
    "REGION_NAMES_PATH = config.get(\"S3\", \"SPARK_INPUT_REGION_NAMES\")\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = config.get(\"AWS\", \"AWS_ACCESS_KEY_ID\")\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = config.get(\"AWS\", \"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.config(\n",
    "        \"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\"\n",
    "    )\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Price Paid Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_unique_identifier: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- date_of_transfer: string (nullable = true)\n",
      " |-- postcode: string (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- old_new: string (nullable = true)\n",
      " |-- duration: string (nullable = true)\n",
      " |-- paon: string (nullable = true)\n",
      " |-- saon: string (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- locality: string (nullable = true)\n",
      " |-- town_city: string (nullable = true)\n",
      " |-- district: string (nullable = true)\n",
      " |-- county: string (nullable = true)\n",
      " |-- ppd_category_type: string (nullable = true)\n",
      " |-- record_status: string (nullable = true)\n",
      "\n",
      "CPU times: user 9.91 ms, sys: 4.22 ms, total: 14.1 ms\n",
      "Wall time: 4.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ppd_df = etl_spark.read_ppd_table(spark, PPD_PATH, PPD_HEADERS)\n",
    "ppd_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(transaction_unique_identifier='{A42E2F04-2538-4A25-94C5-49E29C6C8FA8}', price='18500', date_of_transfer='1995-01-31 00:00', postcode='TQ1 1RY', property_type='F', old_new='N', duration='L', paon='VILLA PARADISO', saon='FLAT 10', street='HIGHER WARBERRY ROAD', locality='TORQUAY', town_city='TORQUAY', district='TORBAY', county='TORBAY', ppd_category_type='A', record_status='A')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppd_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.27 ms, sys: 3.07 ms, total: 6.34 ms\n",
      "Wall time: 18.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25233169"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ppd_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Postcodes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- postcode: string (nullable = true)\n",
      " |-- positional_quality_indicator: string (nullable = true)\n",
      " |-- eastings: string (nullable = true)\n",
      " |-- northings: string (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- nhs_regional_ha_code: string (nullable = true)\n",
      " |-- nhs_ha_code: string (nullable = true)\n",
      " |-- admin_county_code: string (nullable = true)\n",
      " |-- admin_district_code: string (nullable = true)\n",
      " |-- admin_ward_code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "post_df = etl_spark.read_postcodes(spark, POSTCODE_PATH)\n",
    "post_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(postcode='B1  1AY', positional_quality_indicator='10', eastings='406523', northings='286448', country_code='E92000001', nhs_regional_ha_code='E19000001', nhs_ha_code='E18000005', admin_county_code=None, admin_district_code='E08000025', admin_ward_code='E05011151')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.06 ms, sys: 1.52 ms, total: 2.58 ms\n",
      "Wall time: 1.04 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1706904"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "post_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Steps\n",
    "There are a few steps we need to undertake to clean the datasets:\n",
    "\n",
    "1. Making postcodes consistent in both datasets, as it is our join key. The simplest approach is removing white spaces and uppercasing the text (see `utils._normalise_postcode`).\n",
    "2. Filtering out \"additional\" transactions, i.e. those that have PPD type \"B\", as they seem to be transactions of a different nature (not sales).\n",
    "2. Cleaning address in the sales dataset. For the chosen analysis level, the granular address of a property is not important and is used for archiving purposes only. Therefore, one solution is to collapse primary and secondary lines of address with the street name (see `utils._normalise_address`).\n",
    "3. Translating abbreviations into region/country names. This is done to assist the end users to easier work with the database (see `utils._add_country_name`, `utils._add_region_name`).\n",
    "4. Converting British National Grid (BNG) eastings and northings into latitude and longitude, since many map visualizations use those instead of BNG (see `utils._add_location`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "While designing a data model for the project, it was important to decide which level of analysis users would perform. It could be property-level, and thus a property would be modeled individually based on its unique address. It could be area-level, where properties would be aggregated based on postal code. At the data lake stage, we choose the former approach, saving details of both postcodes and individual addresses.\n",
    "\n",
    "For this project and its end user needs, a **star schema** seems to be most appropriate:\n",
    "1. The data is highly structured, so a relational database/SQL approach is viable, providing high performance for JOIN queries needed for this type of analysis. The large size of the dataset can be mitigated by using scalable solutions like AWS Redshift.\n",
    "2. The data naturally falls into facts/dimensions division: we have sales and information about each element (dimensions) of a sale.\n",
    "\n",
    "Below is the selected target data model, with `Sale` being the fact table with `Time`, `Property`, `Postcode` dimensions:\n",
    "\n",
    "<img src=\"uml.png\" alt=\"UML Diagram of Star Schema\" width=\"500\"/>\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "To transform the raw datasets into the chosen data model, we use the following pipeline:\n",
    "1. Read the CSV files into Apache Spark dataframes.\n",
    "2. Apply the cleaning/transformation steps, extract fact and dimension tables into separate parquet files.\n",
    "3. Read the parquet files into Redshift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Pipelines\n",
    "\n",
    "Airflow is used to orchestrate the pipelines:\n",
    "\n",
    "<img src=\"dag_flow.png\" alt=\"Airflow DAG\" width=\"100%\"/>\n",
    "\n",
    "#### 4.1 Create the data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 153 ms, sys: 33.6 ms, total: 186 ms\n",
      "Wall time: 1.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ppd_tables = etl_spark.get_ppd_tables(spark, PPD_PATH, PPD_HEADERS)\n",
    "postcode_tables = etl_spark.get_postcode_tables(spark, POSTCODE_PATH, REGION_NAMES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_info(df):\n",
    "    df.printSchema()\n",
    "    df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "property\n",
      "root\n",
      " |-- property_address: string (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- is_new: boolean (nullable = true)\n",
      " |-- duration: string (nullable = true)\n",
      "\n",
      "+--------------------+-------------+------+---------+\n",
      "|    property_address|property_type|is_new| duration|\n",
      "+--------------------+-------------+------+---------+\n",
      "|A'BECKET GARDENS; 46|semi-detached| false| freehold|\n",
      "|AALBORG PLACE; MI...|         flat|  true|leasehold|\n",
      "|   AALTEN AVENUE; 22|     detached| false| freehold|\n",
      "|    ABBATT CLOSE; 57|     terraced| false| freehold|\n",
      "|    ABBAY STREET; 27|     terraced| false| freehold|\n",
      "+--------------------+-------------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "time\n",
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      "\n",
      "+----------+----+-----+---+----+-------+\n",
      "|      date|year|month|day|week|weekday|\n",
      "+----------+----+-----+---+----+-------+\n",
      "|1995-12-01|1995|   12|  1|  48|      6|\n",
      "|1995-10-24|1995|   10| 24|  43|      3|\n",
      "|1995-09-03|1995|    9|  3|  35|      1|\n",
      "|1996-12-22|1996|   12| 22|  51|      1|\n",
      "|1997-12-11|1997|   12| 11|  50|      5|\n",
      "+----------+----+-----+---+----+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "sale\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- postcode: string (nullable = true)\n",
      " |-- property_address: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n",
      "+--------------------+-----+----------+--------+--------------------+----+-----+\n",
      "|                  id|price|      date|postcode|    property_address|year|month|\n",
      "+--------------------+-----+----------+--------+--------------------+----+-----+\n",
      "|A42E2F04-2538-4A2...|18500|1995-01-31|  TQ11RY|HIGHER WARBERRY R...|1995|    1|\n",
      "|1BA349E3-2579-40D...|73450|1995-10-09|  L267XJ|      CATKIN ROAD; 6|1995|   10|\n",
      "|E5B50DCB-BC7A-4E5...|59000|1995-03-31| BH122AE|      ALDER ROAD; 28|1995|    3|\n",
      "|81E50116-D675-4B7...|31000|1995-12-04| IP130DR|THE STREET; NONSU...|1995|   12|\n",
      "|B97455B9-75CB-40B...|95000|1995-09-22| WS140BE|HALL LANE; FOX CO...|1995|    9|\n",
      "+--------------------+-----+----------+--------+--------------------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for table, _df in ppd_tables.items():\n",
    "    print(table)\n",
    "    show_info(_df)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postcode\n",
      "root\n",
      " |-- postcode: string (nullable = true)\n",
      " |-- district: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      "\n",
      "+--------+----------+-------+----------+---------+\n",
      "|postcode|  district|country| longitude| latitude|\n",
      "+--------+----------+-------+----------+---------+\n",
      "|   B11AY|Birmingham|England|-3.8408782|2.5912867|\n",
      "|   B11BA|Birmingham|England|-3.8377116|2.5907912|\n",
      "|   B11BB|Birmingham|England|-3.8400989| 2.596163|\n",
      "|   B11BD|Birmingham|England| -3.839387|   2.5943|\n",
      "|   B11BE|Birmingham|England|-3.8386576|2.5929344|\n",
      "+--------+----------+-------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for table, _df in postcode_tables.items():\n",
    "    print(table)\n",
    "    show_info(_df)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Data quality checks are governed by the corresponding Airflow operator that currently ensures that the tables are not empty. Moreover, there are integrity constraints in Redshift, like `NOT NULL` and data type checks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "##### Sale\n",
    "\n",
    "Data item | Explanation\n",
    "----------|-------------------------------\n",
    "id | Unique identifier of a sale transaction, the same as used in the Price Paid Dataset.\n",
    "price | Price paid in the transaction, in British Pounds.\n",
    "date | Date of the transaction.\n",
    "property_id | Numeric ID of the property type being sold, corresponds to IDs in the Property table.\n",
    "postcode | UK postcode of the property being sold.\n",
    "address | Additional locality information about the concrete property being sold.\n",
    "\n",
    "##### Postcode\n",
    "Data item | Explanation\n",
    "----------|-------------------------------\n",
    "code | Postal code, uppercase with no whitespaces.\n",
    "district | District name where the postcode is located.\n",
    "country | Country name where the postcode is located.\n",
    "latitude | Latitude of the postcode.\n",
    "longitude | Longitude of the postcode.\n",
    "\n",
    "##### Time\n",
    "Data item | Explanation\n",
    "----------|-------------------------------\n",
    "date | Date.\n",
    "day | Day of the date.\n",
    "week | Week of the year in the date.\n",
    "month | Month of the date.\n",
    "year | Year of the date.\n",
    "weekday | Day of the week in the date.\n",
    "\n",
    "##### Property\n",
    "Data item | Explanation\n",
    "----------|-------------------------------\n",
    "id | Unique property type id.\n",
    "type | Property type (e.g. flat, detached).\n",
    "is_new | Whether the property is being sold for the first time.\n",
    "duration | Whether this is a leased or owned property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Conclusion (Project Write Up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project's goal was to enable data analysts to perform complex queries on a data warehouse containing information about property sales in the UK, as well as the geographical location of the houses. During the project scoping stage, the data sources were identified: 15 years of Price Paid Dataset entries and all UK postal code locations, both summing to 26M rows. A target data model was defined using a star schema, where house sales constitued the fact table, with time, postcodes, and property details being dimensions. To achieve non-trivial transformations, raw data was first put into a data lake and undergone processing before being copied into the final database.\n",
    "\n",
    "The following tools and technologies were selected for the project:\n",
    "- AWS Redshift: Since the data is already highly structured, it was reasonable to leverage SQL solutions to address the need of JOINs in potential analyses. The large size of datasets is mitigated by Redshift's scalability.\n",
    "- Apache Spark: Used to additionally transform data before uploading it into Redshift. Spark was chosen to do the \"heavy-lifting\" of processing, since the transformations included non-trivial functions like converting BNG coordinates into latitude and longitude.\n",
    "- All files were stored in AWS S3 to ensure the highest network speed between AWS tools.\n",
    "  \n",
    "The sales data was planned to be updated monthly, following the original price dataset's release schedule. The postal code data might be updated either ad-hoc after news that postcodes have been reallocated, or once a year as a routine maintenance (since such events happen relatively rarely).\n",
    "\n",
    "The following scenarios were considered:\n",
    "- If the data was to increase by 100x, it would still be possible to store it in Redshift, especially if the cluster size is increased.\n",
    "- If the data populated a dashboard that must be updated on a daily basis by 7am every day, Airflow could be set to update the data daily before 7am.\n",
    "- If the database needed to be accessed by 100+ people, Redshift would need to be scaled, which is also doable in AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "postgres-models",
   "language": "python",
   "name": "postgres-models"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
